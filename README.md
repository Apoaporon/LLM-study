# LLM-study

# function calling
事前に定義した関数を必要に応じて呼び出し、関数の実行結果を元に回答を生成させるもの。

ユーザの質問から文章を解析して必要な関数を呼び出して、必要情報を付加して解答を生成する

- メリット
    - 外部のデータベースにアクセス結果をもとに回答できるようになる
    - OpenAIと外部のシステム連携をミスなく正確に行うことができるようになります。(この機能ができるまでは安定していなかった、プロンプトエンジニアリングによる調整が必要だった)

- 流れ
    1. 関数定義
    2. プロンプトと関数の送信（（AIが）質問に必要な関数を選び、引数を作成する）
        - ユーザのプロンプトと同時に作成した関数のリストをAPIに送信
    3. モデルの判断
        - モデルで学習された内容のみで解答を作成するか、関数を使うべきかを判断する
    4. 関数の実行（（プログラムが）関数を実行）
        - もし、関数を実行すべきと判断した場合は、関数を実行するための引数を辞書型のデータで返す。そのデータをもとにクライアント側で関数を実行して、その結果を返す
    5. 関数の出力結果に基づく応答　（（AIが）関数結果も入力に入れて質問に回答する）
        - 関数の実行結果を加えて、最終的な解答を生成して、ユーザに返す

<center><img src="langchain-model.png" width=75%></center>

- 補足
    - 4の時点での関数実行結果を5に使わなければいけないわけではない。例えば、4の結果をもとに別の処理を走らせるなどしてもOKらしい→その結果をさらに2回目の回答情報に追加することもできるかも？


### ざっくりイメージ（引用）
<img src="image.png" width=50%>


## function callingの種類
### 旧式のfunction calling
1つのリクエストに対して、一つの関数を利用するかしないかを判定して、function callingを行う([参考](https://www.tdi.co.jp/miso/openai-function-calling-gpt))
### Parallel function calling (並列実行)
1つのリクエストに対して、複数の関数を選択、実行してレスポンスを生成してくれる　← こちらが主流になりつつある。

toolsという引数が増え、今後のアップデートも期待できそう（[参考](https://book.st-hakky.com/data-science/open-ai-parallel-function-calling/)）



# RAG(Retrieval-Augmented Generation)
RAG（Retrieval-Augmented Generation）システムは、LLM（Large Language Model）と呼ばれる大規模言語モデルと、Retrieval（文書検索モデル）を組み合わせた、質問応答（QA）を行うための自然言語処理（NLP）システムの一種です。その主な機能は、大量の情報源から必要な情報を取り出し、その情報に基づいて回答を生成することです。
RAGはファインチューニングとは異なり、 LLMのパラメータを調整するわけではなく、 LLMの入力するデータの内容を調整する手法で、プロンプトエンジニアリングに含まれるもの


### ざっくりイメージ([引用](https://storialaw.jp/blog/9916))
<img src="image-2.png" width=50%>

- メリット
    -  LLMが処理できるデータは訓練データの中だけですが、RAGを用いることでリアルタイムで外部の情報源にアクセスし、より正確で豊富な回答を生成することが可能になる。
    - 
# LangChain